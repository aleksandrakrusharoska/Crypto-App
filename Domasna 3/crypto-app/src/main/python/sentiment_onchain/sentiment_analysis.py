import os
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
import json

import requests
import numpy as np
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
from cachetools import TTLCache
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# ===================== CONFIGURATION =====================

class SentimentConfig:
    NEWSAPI_KEY = os.getenv("NEWSAPI_KEY")
    BERT_MODEL = "ProsusAI/finbert"
    CACHE_TTL = 300  # 5 –º–∏–Ω—É—Ç–∏
    REQUEST_TIMEOUT = 15


# –ö–µ—à –∑–∞ sentiment —Ä–µ–∑—É–ª—Ç–∞—Ç–∏
sentiment_cache = TTLCache(maxsize=500, ttl=SentimentConfig.CACHE_TTL)


# ===================== BERT MODEL =====================

class FinBERTSentimentAnalyzer:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        try:
            logger.info("üîÑ –í—á–∏—Ç—É–≤–∞—ö–µ –Ω–∞ FinBERT –º–æ–¥–µ–ª...")
            self.tokenizer = AutoTokenizer.from_pretrained(SentimentConfig.BERT_MODEL)
            self.model = AutoModelForSequenceClassification.from_pretrained(SentimentConfig.BERT_MODEL)
            self.pipeline = pipeline(
                task="text-classification",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            self._initialized = True
            logger.info("‚úÖ FinBERT –º–æ–¥–µ–ª —É—Å–ø–µ—à–Ω–æ –≤—á–∏—Ç–∞–Ω")

            device_info = "GPU (CUDA)" if torch.cuda.is_available() else "CPU"
            logger.info(f"üìä –ö–æ—Ä–∏—Å—Ç–∏: {device_info}")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–º–æ–∂–µ –¥–∞ —Å–µ –≤—á–∏—Ç–∞ FinBERT: {e}")
            self.pipeline = None
            self._initialized = True

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–∞—ò –µ–¥–µ–Ω —Ç–µ–∫—Å—Ç (–≤–µ—Å—Ç) –∑–∞ sentiment

        –í—Ä–∞—ú–∞:
        {
            "label": "POSITIVE" | "NEGATIVE" | "NEUTRAL",
            "score": 0.0-1.0,
            "confidence": 0.0-1.0
        }
        """
        if not self.pipeline:
            logger.warning("‚ö†Ô∏è FinBERT –Ω–µ –µ –¥–æ—Å—Ç–∞–ø–µ–Ω, –≤—Ä–∞—ú–∞ NEUTRAL")
            return {
                "label": "NEUTRAL",
                "score": 0.5,
                "confidence": 0.0
            }

        try:
            text_truncated = text[:512]

            result = self.pipeline(text_truncated)[0]

            # FinBERT –≤—Ä–∞—ú–∞ "positive", "negative", "neutral"
            label_mapping = {
                "positive": "POSITIVE",
                "negative": "NEGATIVE",
                "neutral": "NEUTRAL"
            }

            label = label_mapping.get(result["label"].lower(), "NEUTRAL")
            score = float(result["score"])

            return {
                "label": label,
                "score": score,
                "confidence": score
            }
        except Exception as e:
            logger.error(f"‚ùå Sentiment –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—É—Å–ø–µ—à–Ω–∞: {e}")
            return {
                "label": "NEUTRAL",
                "score": 0.5,
                "confidence": 0.0
            }


finbert_analyzer = FinBERTSentimentAnalyzer()


# ===================== NEWSAPI =====================

def fetch_news_articles(symbol: str, days: int = 30, max_articles: int = 100) -> Optional[list]:
    """
    –ü—Ä–µ–≤–∑–µ–º–∏ –≤–µ—Å—Ç–∏ –æ–¥ NewsAPI –∑–∞ –¥–∞–¥–µ–Ω –∫–æ–∏–Ω

    Args:
        symbol: "BTC", "ETH", –∏—Ç–Ω.
        days: –ö–æ–ª–∫—É –¥–µ–Ω–æ–≤–∏ –Ω–∞–∑–∞–¥
        max_articles: –ú–∞–∫—Å–∏–º–∞–ª–µ–Ω –±—Ä–æ—ò –≤–µ—Å—Ç–∏

    –í—Ä–∞—ú–∞:
    [
        {
            "title": "...",
            "description": "...",
            "source": "CoinDesk",
            "publishedAt": "2024-12-14T12:00:00Z",
            "url": "..."
        },
        ...
    ]
    """
    if not SentimentConfig.NEWSAPI_KEY:
        logger.warning("‚ö†Ô∏è NEWSAPI_KEY –Ω–µ –µ –ø–æ—Å—Ç–∞–≤–µ–Ω")
        return []

    try:
        end = datetime.utcnow()
        start = end - timedelta(days=days)

        params = {
            "q": symbol,
            "from": start.date().isoformat(),
            "to": end.date().isoformat(),
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": max_articles,
            "apiKey": SentimentConfig.NEWSAPI_KEY
        }

        logger.info(f"üîÑ –ü—Ä–µ–≤–∑–µ–º–∞—ö–µ –≤–µ—Å—Ç–∏ –∑–∞ {symbol}...")
        r = requests.get(
            "https://newsapi.org/v2/everything",
            params=params,
            timeout=SentimentConfig.REQUEST_TIMEOUT
        )
        r.raise_for_status()

        data = r.json()
        articles = data.get("articles", [])
        logger.info(f"‚úÖ –ü—Ä–µ–≤–∑–µ–º–µ–Ω–∏ {len(articles)} –≤–µ—Å—Ç–∏ –∑–∞ {symbol}")

        return articles
    except Exception as e:
        logger.error(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –ø—Ä–µ–≤–∑–µ–º–∞—ö–µ –≤–µ—Å—Ç–∏: {e}")
        return []


# ===================== SENTIMENT –ê–ù–ê–õ–ò–ó–ê =====================

def analyze_sentiment_for_symbol(symbol: str, days: int = 30, max_articles: int = 100) -> Dict[str, Any]:
    """
    –ì–õ–ê–í–ù–ê –§–£–ù–ö–¶–ò–à–ê: –ê–Ω–∞–ª–∏–∑–∏—Ä–∞—ò sentiment –∑–∞ —Å–∏–º–±–æ–ª

    1. –ü—Ä–µ–≤–∑–µ–º–∏ –≤–µ—Å—Ç–∏ –∑–∞ —Å–∏–º–≤–æ–ª
    2. –ê–Ω–∞–ª–∏–∑–∏—Ä–∞—ò sentiment –Ω–∞ —Å–µ–∫–æ—ò–∞ –≤–µ—Å—Ç —Å–æ FinBERT
    3. –ö–∞–ª–∫—É–ª–∏—Ä–∞—ò –ø—Ä–æ—Å–µ–∫–æ—Ç
    4. –í—Ä–∞—ú–∞—ò —Ä–µ–∑—É–ª—Ç–∞—Ç

    –í—Ä–∞—ú–∞:
    {
        "symbol": "BTC",
        "timestamp": "2024-12-14T...",
        "articles_analyzed": 50,
        "positive_ratio": 0.65,
        "negative_ratio": 0.25,
        "neutral_ratio": 0.10,
        "average_score": 0.72,
        "final_sentiment_score": 0.61,  # weighted
        "samples": [
            {
                "title": "Bitcoin hits...",
                "sentiment": "POSITIVE",
                "score": 0.92,
                "source": "CoinDesk",
                "publishedAt": "2024-12-14T..."
            },
            ...
        ],
        "interpretation": "‚≠ê –ü–û–ó–ò–¢–ò–í–ù–û - –í–µ—Å—Ç–∏ —Å–µ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏"
    }
    """
    cache_key = f"sentiment:{symbol}:{days}"

    # –ü—Ä–æ–≤–µ—Ä–∏ –∫–µ—à
    if cache_key in sentiment_cache:
        logger.info(f"üíæ Sentiment –∑–∞ {symbol} –µ –æ–¥ –∫–µ—à")
        return sentiment_cache[cache_key]

    # –ü—Ä–µ–≤–∑–µ–º–∏ –≤–µ—Å—Ç–∏
    articles = fetch_news_articles(symbol, days, max_articles)

    if not articles:
        logger.warning(f"‚ö†Ô∏è –ù–µ–º–∞ –≤–µ—Å—Ç–∏ –∑–∞ {symbol}")
        return _default_sentiment(symbol)

    # –ê–Ω–∞–ª–∏–∑–∏—Ä–∞—ò sentiment –Ω–∞ —Å–µ–∫–æ—ò–∞ –≤–µ—Å—Ç
    sentiments = []

    logger.info(f"üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä–∞—ö–µ sentiment –∑–∞ {len(articles)} –≤–µ—Å—Ç–∏...")
    for i, article in enumerate(articles):
        title = article.get("title", "").strip()

        if not title:
            continue

        # FinBERT –∞–Ω–∞–ª–∏–∑–∞
        sentiment = finbert_analyzer.analyze_text(title)

        sentiments.append({
            "title": title,
            "sentiment": sentiment["label"],
            "score": sentiment["score"],
            "source": article.get("source", {}).get("name", "Unknown"),
            "publishedAt": article.get("publishedAt")
        })

        if (i + 1) % 10 == 0:
            logger.info(f"  üìä –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ {i + 1}/{len(articles)}...")

    if not sentiments:
        return _default_sentiment(symbol)

    # –ö–∞–ª–∫—É–ª–∏—Ä–∞—ò —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    positive_count = sum(1 for s in sentiments if s["sentiment"] == "POSITIVE")
    negative_count = sum(1 for s in sentiments if s["sentiment"] == "NEGATIVE")
    neutral_count = sum(1 for s in sentiments if s["sentiment"] == "NEUTRAL")

    total = len(sentiments)
    positive_ratio = positive_count / total
    negative_ratio = negative_count / total
    neutral_ratio = neutral_count / total

    # –ü—Ä–æ—Å–µ—á–µ–Ω score
    average_score = float(np.mean([s["score"] for s in sentiments]))

    # –§–∏–Ω–∞–ª–µ–Ω sentiment score (weighted)
    # POSITIVE = +0.5, NEGATIVE = -0.5, NEUTRAL = 0
    weights = []
    for s in sentiments:
        if s["sentiment"] == "POSITIVE":
            weights.append(0.5 * s["score"])
        elif s["sentiment"] == "NEGATIVE":
            weights.append(-0.5 * s["score"])
        else:
            weights.append(0.0)

    final_sentiment_score = float(np.mean(weights)) + 0.5  # –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞—ò 0-1

    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–∞—ö–µ
    if positive_ratio >= 0.6:
        interpretation = "‚≠ê‚≠ê –°–∏–ª–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ - –í–µ—Å—Ç–∏—Ç–µ —Å–µ –º–Ω–æ–≥—É –ø–æ–∑–∏—Ç–∏–≤–Ω–∏"

    elif positive_ratio >= 0.5:
        interpretation = "‚≠ê –ü–æ–∑–∏—Ç–∏–≤–Ω–æ - –í–µ—Å—Ç–∏—Ç–µ —Å–µ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏"

    elif neutral_ratio >= 0.5:
        interpretation = "‚ö™ –ù–µ—É—Ç—Ä–∞–ª–Ω–æ - –í–µ—Å—Ç–∏—Ç–µ —Å–µ –ø—Ä–µ—Ç–µ–∂–Ω–æ –Ω–µ—É—Ç—Ä–∞–ª–Ω–∏"

    elif negative_ratio >= 0.4:
        interpretation = "üëé –ù–µ–≥–∞—Ç–∏–≤–Ω–æ - –í–µ—Å—Ç–∏—Ç–µ —Å–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏"

    else:
        interpretation = "üëéüëé –°–∏–ª–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ - –í–µ—Å—Ç–∏—Ç–µ —Å–µ –º–Ω–æ–≥—É –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏"

    result = {
        "symbol": symbol.upper(),
        "timestamp": datetime.utcnow().isoformat(),
        "articles_analyzed": total,
        "positive_ratio": round(positive_ratio, 3),
        "negative_ratio": round(negative_ratio, 3),
        "neutral_ratio": round(neutral_ratio, 3),
        "average_score": round(average_score, 3),
        "final_sentiment_score": round(final_sentiment_score, 3),
        "samples": sentiments[:5],  # –¢–æ–ø 5 –≤–µ—Å—Ç–∏
        "interpretation": interpretation
    }

    # –ö–µ—à —Ä–µ–∑—É–ª—Ç–∞—Ç
    sentiment_cache[cache_key] = result
    logger.info(f"‚úÖ Sentiment –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤—Ä—à–µ–Ω–∞ –∑–∞ {symbol}")

    return result


def _default_sentiment(symbol: str) -> Dict[str, Any]:
    """Default —Ä–µ–∑—É–ª—Ç–∞—Ç –∫–æ–≥–∞ –Ω–µ–º–∞ –≤–µ—Å—Ç–∏"""
    return {
        "symbol": symbol.upper(),
        "timestamp": datetime.utcnow().isoformat(),
        "articles_analyzed": 0,
        "positive_ratio": 0.5,
        "negative_ratio": 0.5,
        "neutral_ratio": 0.0,
        "average_score": 0.5,
        "final_sentiment_score": 0.5,
        "samples": [],
        "interpretation": "‚ö†Ô∏è –ù–ï–î–û–°–¢–ê–ü–ù–ò –í–ï–°–¢–ò - –ù–µ–º–∞ –¥–æ–≤–æ–ª–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    }


# ===================== SETUP =====================

def setup_sentiment_logging():
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–∞—ò logging –∑–∞ sentiment –º–æ–¥—É–ª"""
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - sentiment - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)


# ===================== TEST =====================

if __name__ == "__main__":
    setup_sentiment_logging()

    print("\n" + "=" * 70)
    print("üîç SENTIMENT –ê–ù–ê–õ–ò–ó–ê - –¢–ï–°–¢")
    print("=" * 70)

    # –¢–µ—Å—Ç–∏—Ä–∞—ò BTC
    result = analyze_sentiment_for_symbol("BTC")
    print(json.dumps(result, indent=2, default=str))

    print("\n" + "=" * 70)

    # –¢–µ—Å—Ç–∏—Ä–∞—ò ETH
    result = analyze_sentiment_for_symbol("ETH")
    print(json.dumps(result, indent=2, default=str))
